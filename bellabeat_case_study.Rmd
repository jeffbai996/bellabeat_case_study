---
title: "Bellabeat Case Study - How Can a Wellness Technology Company Play It Smart?"
author: "Jeff Bai"
date: '2022-07-14'
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
```

This case study is a capstone project for the Google Data Analytics Professional Certificate. The following analysis was completed for Course 8 - Case Study 2. For the purposes of this case study, I assume the role of a Junior Data Analyst working on the Marketing Analytics team at Bellabeat, a high-tech company that manufactures health-focused smart products for women.

# Case Introduction
### About Bellabeat
Bellabeat was founded in 2013 by Urška Sršen and Sando Mur. Leveraging Sršen's background as an artist, Bellabeat has developed beautifully designed technology that informs and inspires women around the world. By collecting data on activity, sleep, stress, and reproductive health, Bellabeat empowers women with actionable insights into their health and habits. 

By 2016, Bellabeat achieved a worldwide presence and launched a variety of products. These products are available through online retailers or through Bellabeat's e-commerce channel. To engage their customer base, Bellabeat has invested in traditional advertising media (radio, billboards, print, TV), in addition to extensive digital marketing (Google Ads/Search, Facebook, Instagram, YouTube).

Bellabeat has 5 main products:

1. **Bellabeat app**  
The Bellabeat app provides users with health data related to their activity, sleep, stress, menstrual cycle, and mindfulness habits. This data can help users better understand their current habits and make healthy decisions. The Bellabeat app connects to their line of smart wellness products.
2. **Leaf**  
Bellabeat’s classic wellness tracker can be worn as a bracelet, necklace, or clip. The Leaf tracker connects to the Bellabeat app to track activity, sleep, and stress.
3. **Time**  
This wellness watch combines the timeless look of a classic timepiece with smart technology to track user activity, sleep, and stress. The Time watch connects to the Bellabeat app to provide you with insights into your daily wellness.
4. **Spring**  
This is a water bottle that tracks daily water intake using smart technology to ensure that you are appropriately hydrated throughout the day. The Spring bottle connects to the Bellabeat app to track your hydration levels.
5. **Bellabeat membership**  
Bellabeat also offers a subscription-based membership program for users. Membership gives users 24/7 access to fully personalized guidance on nutrition, activity, sleep, health and beauty, and mindfulness based on their lifestyle and goals.

### Scenario and Key Questions
Chief Creative Officer Urška Sršen knows that an analysis of Bellabeat's available consumer data would reveal more opportunities for growth. She has tasked the Marketing Analytics team with analyzing usage data from a public dataset in order to gain insight on how people are using smart fitness trackers. With this information, Sršen hopes to receive high-level recommendations for how these trends can inform Bellabeat's marketing strategy for their product moving forward.

Our team will rely on three key guiding questions for the direction of our analysis:

1. **What are some trends in smart device usage?**
2. **How could these trends apply to Bellabeat customers?**
3. **How could these trends help influence Bellabeat marketing strategy?**

### Stakeholders
**Urška Sršen**  
Bellabeat's co-founder and Chief Creative Officer

**Sando Mur**  
Mathematician and Bellabeat's cofounder; key member of the Bellabeat executive team

**Bellabeat Marketing Analytics team**  
A team of data analysts responsible for collecting, analyzing, and reporting data that informs Bellabeat's marketing strategy

### Scope of Work
Our team will produce a report with the following deliverables:

* A clear summary of the business task
* A description of all data sources used
* Documentation of any cleaning or manipulation of data
* A summary of the analysis
* Supporting visualizations and key findings
* Top high-level content recommendations based on analysis

# Deliverables

## Business Task
With smart devices a key pillar of Bellabeat's business, our team aims to dive deep into usage data, drawing insights and conclusions from how smart devices are used in general, in order to inform Bellabeat's future marketing strategy and drive future growth.

## Data Sources
The dataset used in this analysis is [Fitbit Fitness Tracker Data](https://www.kaggle.com/datasets/arashnic/fitbit), sourced from the personal fitness tracker data of 30 Fitbit users. It contains metrics such as physical activity output, heart rate, sleep patterns, and daily activity. This dataset was made available for this analysis through Mobius, a Kaggle contributor. The dataset is in the public domain, under the terms of Creative Commons CC0 1.0 Universal.

### ROCCC analysis
To evaluate the quality of this dataset from a high-level perspective, we will use the ROCCC system.

**Reliable?**  
A large part of the dataset is sourced from usage data from fitness trackers. Only one subset - weight log info - relies on self-reporting. Since Fitbit is a very well-established maker of fitness trackers, we will assume that the data generated by their devices **is reliable and accurate**. 

One important caveat to note is, since we have little information on the 30 individuals represented in the dataset (no age, gender, etc.), results may be skewed if the sample is not representative of the general population of smart tracker users.

**Original?**  
This dataset is **not original** to Fitbit, as it was collected by third-parties via Amazon Mechanical Turk.

**Comprehensive?**  
The dataset includes activity rate, calories burned, exercise intensity, steps taken, heartrate, and sleep duration. The data is further broken down into daily aggregated data, hourly data, and minute-by-minute data. For the purposes of our analysis and in the context of our business task, this dataset **is comprehensive**.

**Current?**  
This dataset date range is from April to May 2016, and is **not current**. At the same time, fitness tracker users' activity patterns are not likely to have changed in the years since the data was collected, and is therefore unlikely to skew our results.

**Cited?**  
This dataset's source, authors, and collection method are well documented and can be found on the dataset's [web page](https://www.kaggle.com/datasets/arashnic/fitbit).

## Data Cleaning
Since our stakeholders desire a high-level analysis of usage trends instead of minute-by-minute activity by individual, we will not include hourly, by-minute, and by-second data for heartrate, intensity, steps, and sleep in our analysis. This leaves us with 6 .csv files:

* dailyActivity_merged.csv
* dailyCalories_merged.csv
* dailyIntensities_merged.csv
* dailySteps_merged.csv
* sleepDay_merged.csv
* weightLogInfo_merged.csv

Exploring the data within Google Sheets, we found that dailyCalories_merged, dailyIntensities_merged, and dailySteps_merged are simply subsets of dailyActivity_merged. We can therefore remove these 3 .csv files without losing any data.

We'll also truncate the titles of our remaining 3 .csv files, removing "_merged" as it is redundant and needlessly makes our data more difficult to manipulate. This leaves us with:

* dailyActivity.csv
* sleepDay.csv
* weightLogInfo.csv

### Cleaning with BigQuery
Before digging deeper with R, we first want to learn more about the attributes of our selected datasets with Google's BigQuery.

***Note**: sleepDay.csv and weightLogInfo.csv were not able to be uploaded to BigQuery in their raw form as the timestamp column could not be parsed, either with auto or manual schemas. Since the data here were collected on a daily basis, we removed the time and reformatted these columns into a MM/DD/YYYY format with Google Sheets before uploading to BigQuery.*

**Distinct IDs**  
Let's take a look at how many distinct IDs (users) are represented in our data:

```{code}
SELECT COUNT(DISTINCT Id) AS distinct_ids
FROM `marine-actor-233222.bellabeat_data.dailyActivity`

SELECT COUNT(DISTINCT Id) AS distinct_ids
FROM `marine-actor-233222.bellabeat_data.sleepDay`

SELECT COUNT(DISTINCT Id) AS distinct_ids
FROM `marine-actor-233222.bellabeat_data.weightLogInfo`
```

Our query results showed that dailyActivity contains **33 unique IDs**. 

sleepDay contains **24** unique IDs.

weightLogInfo contains **8** unique IDs.

sleepDay and weightLogInfo both contain a smaller sample size (n < 30) than is required for the central limit theorem to hold. However, these two pieces of data contain unique metrics (sleep, weight) not represented in the rest of the data. As such, we will retain these two tables for further analysis while keeping in mind their limited statistical power.

**Invalid Records**  
We also want to see how many invalid records exist in our data.

To filter for invalid records in dailyActivity, we select all records where TotalSteps equals 0:

```{code}
SELECT *
FROM `marine-actor-233222.bellabeat_data.dailyActivity` 
WHERE TotalSteps = 0
```

Our query returned 77 rows where TotalSteps equals 0. As expected, these rows also have zero values for TotalDistance, TrackerDistance, LoggedActivitiesDistance, and so on. We will assume these records are a result of the user not tracking their activity for those days. We will drop these rows from our dataset with R later on.

We'll also filter sleepDay and see if there are any records where TotalMinutesAsleep equals 0:

```{code}
SELECT *
FROM `marine-actor-233222.bellabeat_data.sleepDay` 
WHERE TotalMinutesAsleep = 0
```

This query returned no results - we do not have any invalid records in sleepDay.

We then repeat this process for weightLogInfo, using the column WeightKg as our filter:

```{code}
SELECT *
FROM `marine-actor-233222.bellabeat_data.weightLogInfo` 
WHERE WeightKg = 0
```

This query again returned no results.

### Cleaning with R

**Loading required packages**  
We first load the tidyverse and other required packages:

```{r eval=FALSE, message=FALSE}
library(tidyverse)
library(janitor)
library(skimr)
```

**Loading .csv files into data frames**  
We'll then load the .csv files we chose earlier into data frames:

```{r eval=FALSE, message=FALSE}
dailyActivity <- read_csv("datasets/dailyActivity.csv")
sleepDay <- read_csv("datasets/sleepDay.csv")
weightLogInfo <- read_csv("datasets/weightLogInfo.csv")
```

**Converting column names**  
We'll then use the janitor package to change our column names to snake_case from UpperCamelCase to improve readability:

```{r eval=FALSE, message=FALSE} 
dailyActivity <- clean_names(dailyActivity)
sleepDay <- clean_names(sleepDay)
weightLogInfo <- clean_names (weightLogInfo)
```

**Identifying duplicate records**  
Also using the janitor package, let's see if there are any duplicate values in any of our 3 data frames:

```{}
get_dupes(dailyActivity)

No variable names specified - using all columns.

No duplicate combinations found of: id, activity_date, total_steps, total_distance, tracker_distance, logged_activities_distance, very_active_distance, moderately_active_distance, light_active_distance, ... and 6 other variables

get_dupes(sleepDay)

No variable names specified - using all columns.
# A tibble: 6 × 6
          id sleep_day total_sleep_records total_minutes_asleep total_time_in_bed dupe_count
       <dbl> <chr>                   <dbl>                <dbl>             <dbl>      <int>
1 4388161847 5/5/2016                    1                  471               495          2
2 4388161847 5/5/2016                    1                  471               495          2
3 4702921684 5/7/2016                    1                  520               543          2
4 4702921684 5/7/2016                    1                  520               543          2
5 8378563200 4/25/2016                   1                  388               402          2
6 8378563200 4/25/2016                   1                  388               402          2

get_dupes(weightLogInfo)

No variable names specified - using all columns.

No duplicate combinations found of: id, date, weight_kg, weight_pounds, fat, bmi, is_manual_report, log_id
```

It appears we have 6 duplicate records, all in the sleepDay data frame. 

**Removing duplicate and invalid records**  
We want to remove our previously-identified duplicate records, along with the records with 0 total steps as mentioned previously. For this, we will be using the dplyr package, which has already been loaded as part of the tidyverse:

```{r, eval=FALSE, message=FALSE}
sleepDay <- distinct(sleepDay)
```

This operation removed 3 duplicated records from our data frame, as expected.

We'll proceed to removing the 77 records from dailyActivity with total_steps = 0, also using dplyr:

```{r, eval=FALSE, message=FALSE}
dailyActivity <- dailyActivity %>% 
  filter(total_steps > 0)
```

This operation leaves the dailyActivity data frame with 863 rows, as expected.

**Identifying and removing outliers**  
Having removed our unwanted rows, we are now able to run some statistical summaries on our data frames without skewing the results due to zero values and duplicates. For this, we'll use the skimr package we loaded earlier. We call skim_without_charts() first on dailyActivity, with the exception of the id column as it is not numerical data:

```{r eval=FALSE, message=FALSE}
skim_without_charts(dailyActivity, !id)
```

```{}
── Data Summary ────────────────────────
                           Values       
Name                       dailyActivity
Number of rows             863          
Number of columns          15           
_______________________                 
Column type frequency:                  
  character                1            
  numeric                  13           
________________________                
Group variables            None         

── Variable type: character ─────────────────────────────────────────────────────────────────────────────────────────
  skim_variable n_missing complete_rate min max empty n_unique whitespace
1 activity_date         0             1   8   9     0       31          0

── Variable type: numeric ───────────────────────────────────────────────────────────────────────────────────────────
   skim_variable              n_missing complete_rate       mean         sd p0     p25      p50       p75      p100
 1 total_steps                        0             1 8319.      4745.       4 4923    8053     11092.    36019    
 2 total_distance                     0             1    5.98       3.72     0    3.37    5.59      7.90     28.0  
 3 tracker_distance                   0             1    5.96       3.70     0    3.37    5.59      7.88     28.0  
 4 logged_activities_distance         0             1    0.118      0.646    0    0       0         0         4.94 
 5 very_active_distance               0             1    1.64       2.74     0    0       0.410     2.27     21.9  
 6 moderately_active_distance         0             1    0.618      0.905    0    0       0.310     0.865     6.48 
 7 light_active_distance              0             1    3.64       1.86     0    2.34    3.58      4.89     10.7  
 8 sedentary_active_distance          0             1    0.00175    0.00765  0    0       0         0         0.110
 9 very_active_minutes                0             1   23.0       33.6      0    0       7        35       210    
10 fairly_active_minutes              0             1   14.8       20.4      0    0       8        21       143    
11 lightly_active_minutes             0             1  210.        96.8      0  146.    208       272       518    
12 sedentary_minutes                  0             1  956.       280.       0  722.   1021      1189      1440    
13 calories                           0             1 2361.       703.      52 1856.   2220      2832      4900    
```

The attribute that stands out most is the left-skewness of some variables, namely:

* logged_activities_distance
* sedentary_active_distance

This implies that these columns contain mostly zero or near-zero values. For logged_activities_distance, it is likely that most users did not log "activities" while wearing their fitness trackers. For sedentary_active_distance, it appears that very little distance was logged while users were sedentary - a logical conclusion. As such, these two columns have little value for later analysis, and we will remove them from our data frame accordingly.

```{r, eval=FALSE, message=FALSE}
dailyActivity <- select(
    dailyActivity, 
    -c(logged_activities_distance, sedentary_active_distance)
  )
```

Moving on, we'll call skim_without_charts() on sleepDay:

```{r, eval=FALSE, message=FALSE}
skim_without_charts(sleepDay, !id)
```

```
── Data Summary ────────────────────────
                           Values  
Name                       sleepDay
Number of rows             413     
Number of columns          5       
_______________________            
Column type frequency:             
  character                1       
  numeric                  3       
________________________           
Group variables            None    

── Variable type: character ─────────────────────────────────────────────────────
  skim_variable n_missing complete_rate min max empty n_unique whitespace
1 sleep_day             0             1   8   9     0       31          0

── Variable type: numeric ───────────────────────────────────────────────────────
  skim_variable        n_missing complete_rate   mean      sd p0 p25 p50 p75 p100
1 total_sleep_records          0             1   1.12   0.346  1   1   1   1    3
2 total_minutes_asleep         0             1 419.   118.    58 361 433 490  796
3 total_time_in_bed            0             1 459.   127.    61 403 463 526  961
```

This data frame does not appear to have any missing values or statistical oddities warranting action. We'll keep it as it is and move on to the next.

Finally, we will skim weightLogInfo, removing log_id as well:

```{r, eval=FALSE, message=FALSE}
weightLogInfo %>% 
  select(-c(id, log_id)) %>% 
  skim_without_charts()
```

```
── Data Summary ────────────────────────
                           Values    
Name                       Piped data
Number of rows             67        
Number of columns          6         
_______________________              
Column type frequency:               
  character                1         
  logical                  1         
  numeric                  4         
________________________             
Group variables            None      

── Variable type: character ─────────────────────────────────────────────────────
  skim_variable n_missing complete_rate min max empty n_unique whitespace
1 date                  0             1   8   9     0       31          0

── Variable type: logical ───────────────────────────────────────────────────────
  skim_variable    n_missing complete_rate  mean count           
1 is_manual_report         0             1 0.612 TRU: 41, FAL: 26

── Variable type: numeric ───────────────────────────────────────────────────────
  skim_variable n_missing complete_rate  mean    sd    p0   p25   p50   p75  p100
1 weight_kg             0        1       72.0 13.9   52.6  61.4  62.5  85.0 134. 
2 weight_pounds         0        1      159.  30.7  116.  135.  138.  188.  294. 
3 fat                  65        0.0299  23.5  2.12  22    22.8  23.5  24.2  25  
4 bmi                   0        1       25.2  3.07  21.5  24.0  24.4  25.6  47.5
```
We can see that the 'fat' column has a completeness of only 3%, with 65 missing values. We'll remove this column as it is of little value to our later analysis.

We also have a 'log_id' column (filtered out of the data summary) which is not seen in our other data frames. Since it is not numerical data and cannot be used to perform joins, we'll also remove this column.

```{r, eval=FALSE, message=FALSE}
weightLogInfo <- select(
    weightLogInfo, 
    -c(fat, log_id)
  )
```

With our data cleaned, we can move on to the analysis phase of this study.

## Analysis

## Visualizations

## Recommendations
